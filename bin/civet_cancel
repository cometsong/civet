#!/usr/bin/env python

# Copyright 2016 The Jackson Laboratory
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
    cancel a running pipeline instance.  pipeline is identified by 
    passing the path to its log directory, which is unique to that 
    pipeline
"""

from __future__ import print_function

import argparse
import datetime
import sys
import os
import inspect


cmd_folder = os.path.realpath(os.path.abspath(os.path.split(inspect.getfile( inspect.currentframe() ))[0]))
lib_folder = os.path.join(cmd_folder, '../lib')
if lib_folder not in sys.path:
     sys.path.insert(0, lib_folder)

import job_runner.torque as batch_system
import job_runner.common
import exec_modes
import version
    
    
def main():

    version.parse_options()

    parser = argparse.ArgumentParser(prog=os.path.basename(sys.argv[0]))
    parser.add_argument('-r', '--recursive', dest='recursive', action='store_true', help="Run in recursive mode")
    parser.add_argument('dirs', help="Path to pipeline log directory (can be space separated list of dirs)", nargs=argparse.REMAINDER)
    parser.set_defaults(recursive=False)
    
    args = parser.parse_args()

    dir_list_arg = []

    if len(args.dirs) > 0:
        dir_list_arg = args.dirs
    else:
        dir_list_arg.append('.')

    if args.recursive:
        all_log_dirs = []
        for d in dir_list_arg:
            dirs = []
            for root, children, files in os.walk(d):
                if job_runner.common.BATCH_ID_LOG in files:
                    dirs.append(root)
            dirs.sort()
            all_log_dirs += dirs

        
        if len(all_log_dirs) == 0:
            sys.stderr.write("ERROR: no valid log directory found!\n")
            return 1
    
    else:
        all_log_dirs = dir_list_arg
        
    jm = batch_system.JobManager()
    
    for log_dir in all_log_dirs:

        print("\n\nCancelling pipeline with log directory:"
              "\n\t{}\n".format(log_dir))

        # get listing of batch jobs from the pipeline's log directory
        # each line in batch_jobs is [batch_id, job_name, [dependencies]])
        try:
            batch_jobs = job_runner.common.jobs_from_logdir(log_dir)
        except IOError, e:
            print("ERROR: {0} does not appear to be a valid pipeline log "
                  "directory:\n".format(log_dir), file=sys.stderr)
            print("\t{0}\n".format(e), file=sys.stderr)
            sys.exit(1)

        exec_mode = exec_modes.ToolExecModes.get_exec_mode(log_dir)
    
        # we will build a list of unfinished and complete jobs
        unfinished_jobs = []
        complete_jobs = []
    
        # and a dict that lets us lookup the name associated with a job id
        job_name_lookup = {}
    
        for job in batch_jobs:
            job_name_lookup[job[0]] = job[1]
            if not os.path.exists(os.path.join(log_dir, job[1] + job_runner.common.JOB_STATUS_SUFFIX)):
                # for now if the job does not have a status file we assume it is
                # still running or held
                unfinished_jobs.append(job[0])
            else:
                complete_jobs.append(job[0])
    
        if len(unfinished_jobs) == 0:
            print("\tAll jobs are complete, no jobs to cancel.")
            continue

        try:
            # log that the pipeline is being canceled in the pipeline's log
            # directory
            with open(os.path.join(log_dir, job_runner.common.CANCEL_LOG_FILENAME), 'w') as cancel_log:
                cancel_log.write("DATESTAMP=" + datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') + '\n')
    
                # len(unfinished_jobs) > 0, so not all jobs have -status.txt
                # files depending on execution mode we may be able to check the
                # queues and send batch delete requests for them

                if exec_mode == exec_modes.ToolExecModes.BATCH_MANAGED:
                    # civet_managed_batch_master will see the cancel log next
                    # iteration. It will process the cancellation request
                    print("Pipeline is running in managed batch mode, requesting cancellation.")
                    continue

                unknown_jobs = []
                running_jobs = []
                held_jobs = []
                queued_jobs = []

                # check all job id's that don't have a -status.txt file and record their
                # state (running or held) before we do anything else
                for job_id in unfinished_jobs:
                    status = jm.query_job(job_id)
                    if status:
                        if status.state == 'R':
                            running_jobs.append(job_id)
                        elif status.state == 'Q':
                             queued_jobs.append(job_id)
                        elif status.state == 'H':
                            held_jobs.append(job_id)
                        else:
                            # for now treat any other state as running,  
                            # we many want to change this
                            running_jobs.append(job_id)
                    else:
                        unknown_jobs.append(job_id)


                # log status of jobs before issuing any qdels
                cancel_log.write("COMPLETE_JOBS={0}\n".format(complete_jobs))
                cancel_log.write("RUNNING_JOBS={0}\n".format(running_jobs))
                cancel_log.write("PENDING_JOBS={0}\n".format(held_jobs))
                if unknown_jobs:
                    cancel_log.write("UNKNOWN_STATE={0}\n".format(unknown_jobs))
    
        except EnvironmentError:
            print("Unable to open Cancel Log...skipping (maybe you don't own "
                  "this Civet log directory)")
            continue
    
        # delete jobs, starting with held jobs first
        for job_id in held_jobs + queued_jobs + running_jobs:
            rval = jm.delete_job(job_id)
            if rval and rval != batch_system.JobManager.E_UNKNOWN and rval != batch_system.JobManager.E_STATE:
                # rval is not zero and it is not an unknown job id or invalid state
                # error (job may have completed between when we last checked and 
                # now, those return values may be expected)
                print("Error deleting {0} from queue. ({1}).".format(job_id,
                                                                     rval))

        print("Pipeline status prior to cancel:")
        print("Total Pipeline Jobs: {0}".format(len(batch_jobs)))
        print("\tCompleted Jobs: {0}".format(len(complete_jobs)))
        print("\tRunning Jobs: {0}".format(len(running_jobs)))
        print("\tPending Jobs: {0}".format(len(held_jobs)))
        if unknown_jobs:
            print("\tUnknown State (job may have crashed or was previously deleted): {0}".format(len(unknown_jobs)))

        if exec_mode == exec_modes.ToolExecModes.BATCH_STANDARD:
            if len(running_jobs) + len(held_jobs) > 0:
                print("\n\tCancel signal sent for all running and pending jobs")

                for job_id in held_jobs:
                    if not os.path.exists(os.path.join(log_dir, job_name_lookup[job_id] + job_runner.common.JOB_STATUS_SUFFIX)):
                        with open(os.path.join(log_dir, job_name_lookup[job_id] + job_runner.common.JOB_STATUS_SUFFIX), 'w') as summary_file:
                            summary_file.write("canceled=TRUE\n")
                            summary_file.write("state_at_cancel=H")

                for job_id in queued_jobs:
                    if not os.path.exists(os.path.join(log_dir, job_name_lookup[job_id] + job_runner.common.JOB_STATUS_SUFFIX)):
                        summary_file = open(os.path.join(log_dir, job_name_lookup[job_id] + job_runner.common.JOB_STATUS_SUFFIX), 'w')
                        summary_file.write("canceled=TRUE\n")
                        summary_file.write("state_at_cancel=Q")
                        summary_file.close()

            else:
                print("\n\tNo jobs to cancel; pipeline is not running or queued on the cluster.")



if __name__ == '__main__':
    main()
