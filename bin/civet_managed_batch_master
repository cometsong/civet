#!/usr/bin/env python

# Copyright 2017 The Jackson Laboratory
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import print_function

import argparse
import sys
import inspect
import os
import time
import logging

start_time = time.time()

cmd_folder = os.path.realpath(os.path.abspath(os.path.split(inspect.getfile(inspect.currentframe()))[0]))
lib_folder = os.path.join(cmd_folder, '../lib')
if lib_folder not in sys.path:
    sys.path.insert(0, lib_folder)

import version
import utilities
import status

from job_runner.torque import TorqueJobRunner as BatchRunner

from managed_batch.model.session import Session
from managed_batch.model.job import Job
from managed_batch.model.pipeline import Pipeline
from managed_batch.model.status import Status
from managed_batch.controller.utilities import initialize_model, init_statuses, \
    mark_complete_and_release_dependencies, mark_submitted, \
    scan_for_runnable_jobs, count_submitted_jobs, write_batch_id_to_log_dir, dispose_engine

from managed_batch.manager import submit_management_job

_job_manager = status.PipelineStatus.get_job_manager()


def update_job_status(job):
    """
    update the status of a submitted job
    :param job: Job object to update
    :return:
    """
    current_status = job.get_status()
    if current_status == 'Submitted':
        new_status = status.ManagedJobStatus(job.pipeline.log_directory,
                                             job.job_name, job.torque_id,
                                             _job_manager)
        if new_status.state == 'Complete':
            mark_complete_and_release_dependencies(job)
        elif new_status.state == 'Failed' or new_status.state == 'Deleted':
            # for now consider a deleted job as 'failed'
            # the only way for a job to get this state is for the node to
            # crash/reboot while the job is running, otherwise the job
            # epilogue should run and create the status.txt file.
            job.set_status('Failed')
            Session.commit()


def update_jobs():
    logging.debug("Updating Job states")
    submitted_status_id = Status.get_id('Submitted')
    submitted_jobs = Session.query(Job).filter_by(
        status_id=submitted_status_id).all()
    for job in submitted_jobs:
        # update status of job
        update_job_status(job)


def submit_job(job):

    task = {
        'name': job.job_name,
        'threads': job.threads,
        'script_path': job.script_path,
        'stdout_path': job.stdout_path,
        'stderr_path': job.stderr_path,
        'walltime': job.walltime,
        'queue': job.queue,
        'batch_env': job.env,
        'epilogue_path': job.epilog_path,
        'mail_options': job.mail_options,
        'email_list': job.email_list,
        'mem': "{}gb".format(job.mem) if job.mem else None

    }
    batch_id = BatchRunner.submit_managed_job(task)
    # mark job as submitted
    mark_submitted(job, batch_id)


def all_complete():
    is_complete = True
    for pipeline in Session.query(Pipeline):
        if not pipeline.is_complete():
            is_complete = False
            break

    cancel_failed_pipelines()

    return is_complete


def cancel_failed_pipelines():
    """
    cancel any running jobs for pipelines that are considered to be failed
    this mirrors the standard behavior where a failed job sends the qdel
    message to all the other jobs in a pipeline
    :return:
    """
    failed_status_id = Status.get_id("Failed")
    submitted_status_id = Status.get_id("Submitted")
    for pipeline in Session.query(Pipeline).filter_by(status_id=failed_status_id).all():
        print("Detected pipeline failure, canceling all submitted jobs for {}".format(pipeline.name))
        for job in Session.query(Job).filter(Job.pipeline_id == pipeline.id).filter(Job.status_id == submitted_status_id).all():
            logging.debug("deleting {}".format(job.torque_id))
            _job_manager.delete_job(str(job.torque_id))
        #TODO send failure email?


def check_walltime(max_walltime):
    """
    check how long the program has been running, and if we are getting close
    to reaching our maximum walltime return False, otherwise return True
    :param max_walltime: maximum walltime (in minutes)
    :return:
    """
    if max_walltime:
        # get current walltime in minutes
        walltime = (time.time() - start_time) / 60

        # if we have less than 10 minutes to go indicate it's time to resubmit
        if max_walltime - walltime < 10:
            return False
    return True




def main():

    utilities.cleanup_command_line()

    version.parse_options()

    parser = argparse.ArgumentParser()

    parser.add_argument('task_db',
                        help="filename where civet_prepare will store the "
                             "tasks. If the file exists, civet_prepare will "
                             "add new tasks to the file.")
    parser.add_argument('--max-walltime', '-w', type=int, default=None,
                        help="Maximum walltime, in hours, to run. If this "
                             "argument is specified, this program will submit "
                             "a new management job to take over and will exit "
                             "prior to reaching this maximum walltime")
    parser.add_argument('--max-queued', '-m', type=int, default=100,
                        help="maximum number of jobs to have in the running "
                             "state at any one time.")
    parser.add_argument('--queue', '-q', default=None,
                        help="Submission queue to use for management job "
                             "resubmission. Ignored if not combined with "
                             "--max-walltime. [default = TORQUE default]")
    parser.add_argument('--verbose', action='store_true',
                        help="enable verbose mode")

    args = parser.parse_args()

    logging.basicConfig(level=logging.ERROR)

    max_walltime_minutes = args.max_walltime * 60 if args.max_walltime else None

    try:
        print("Opening task database: " + args.task_db)
        Session.session = initialize_model(args.task_db)
    except Exception as e:
        logging.error("Error opening task database: " + e.message)
        sys.exit(1)

    # main loop
    iteration = 0
    while True:

        iteration += 1
        if args.verbose:
            print("Starting iteration {}".format(iteration))

        # update the status of all jobs that currently have the state
        # "Submitted" (these are jobs that were either Queued or Running last
        # iteration
        update_jobs()

        # all_complete() will update Pipeline statuses and return true if they
        # are all now complete
        if all_complete():
            print("\tAll tasks complete.")
            # TODO send completion email? each pipeline already sends a completion email
            print("\tTerminating.")
            break

        if not check_walltime(max_walltime_minutes):
            print("\tApproaching maximum walltime, submitting new management job.")
            dispose_engine()
            job_id = submit_management_job(args.task_db, args.queue,
                                           args.max_walltime, args.max_queued)
            write_batch_id_to_log_dir(job_id)
            print("\tNew management job submitted: " + job_id)
            break


        # find out if we now have some available slots, meaning some of the jobs
        # that had the state Submitted have finished
        available_job_slots = args.max_queued - count_submitted_jobs()
        if args.verbose:
            print("Iteration {}: {} of {} job slots available".format(iteration, available_job_slots, args.max_queued))

        # if we have available slots, see if we have any jobs ready to run
        if available_job_slots:
            logging.info("Iteration {}: {} slots available, looking for eligible jobs".format(iteration, available_job_slots))
            ready_jobs = scan_for_runnable_jobs(limit=available_job_slots)
            print("Iteration {}: Starting {} jobs".format(iteration, len(ready_jobs)))
            for job in ready_jobs:
                logging.info("\tStarting {}".format(job.job_name))
                submit_job(job)


        # done iteration.  Sleep for a little while.
        time.sleep(30)

    # we've broken out of the loop -- either we're done or we submitted another
    # job to take over


if __name__ == "__main__":
    main()
