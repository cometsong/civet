#! /usr/bin/env python

"""
This is an alternate maim program for the pipeline code, that 
collects all the files that need to be validated, and 
generates a json file with their data.  The created file
must be moved into the proper directory for the pipeline's use
(the same directory as the pipeline's main xml file).

Invoke the command with the path to the pipeline's top level 
xml file.  The same cluster software modules as will be 
eventually loaded by the pipeline must be loaded prior to 
execution, and it must be run on one of the cluster's execution
nodes, i.e., it must be run as part of a qsub'd job.
"""

import os, sys, inspect
cmd_folder = os.path.realpath(os.path.abspath(os.path.split(
                              inspect.getfile( inspect.currentframe() ))[0]))
lib_folder = os.path.join(cmd_folder, '../lib')
if lib_folder not in sys.path:
    sys.path.insert(0, lib_folder)

import pipeline_parse as PL
import validity
import version

def main():
    version.parse_options()
    
    # Pass in dummy arguments to let the parse succeed.
    # we're not going to run the pipeline, just parse it. Don't need real
    # files.
    PL.parse_XML(sys.argv[1], ['a', 'a', 'a', 'a'])
    fns = PL.collect_files_to_validate()
    files = validity.FileCollection()
    for fn in fns:
        files.add_file(fn)

    # Figure out whether we're creating a new file or merging with an
    # an existing file.
    name = 'master_file_list'
    if os.path.exists(name):
         print ('Existing file info file found.  '
                'Merging with and updating existing contents.')
         existing = validity.FileCollection()
         existing.from_JSON_file(name)
         existing.merge(files, True)
         files = existing
    files.to_JSON_file(name)

if __name__ == "__main__":
    main()
